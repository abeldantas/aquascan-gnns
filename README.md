# Aquascan: A Digital Twin Architecture for Marine Motion Modeling

## Overview
Aquascan is a simulation environment for modeling spatiotemporal behavior of marine life via a distributed ocean twin simulation. The system simulates a network of IoT devices (Œµ-nodes) in the water, relay nodes (œÉ-nodes), and marine entities (Œ∏-contacts).

The simulation focuses on a marine monitoring network deployed in a rectangular offshore strip measuring 30km along the coastline and extending 16km seaward, from 6km to 22km offshore. This area is designed to monitor marine wildlife corridors and fishing vessel activity.

## Current Status
- ‚úÖ Project structure and core architecture
- ‚úÖ Hexagonal grid node deployment
- ‚úÖ Ocean current simulation with Perlin noise
- ‚úÖ Marine entity motion models (Brownian and sinusoidal)
- ‚úÖ Detection and communication protocols
- ‚úÖ Delaunay-Voronoi mesh network topology for optimal connectivity
- ‚úÖ Real-time visualization with Bokeh
- ‚úÖ Detection visualization (color change for detected entities)
- ‚úÖ Network connections visualization (permanent and intermittent)
- ‚úÖ Configuration management with OmegaConf
- ‚úÖ Module-based architecture with proper imports
- ‚úÖ Unit testing framework
- ‚úÖ Data persistence with HDF5 snapshots
- ‚úÖ **Trajectory visualization and analysis**
- ‚úÖ **Optimized data generation pipeline (5-tick intervals)**
- ‚úÖ **Kalman filter baseline implementation**
- ‚¨ú GNN implementation for prediction
- ‚¨ú Comparison with Kalman filter baseline

## Key Findings from Visualization Analysis

Through trajectory visualization with 48-frame grids, we determined:
- **Optimal snapshot interval**: 5 ticks (640 seconds) at x128 speed
- **Efficient storage**: ~2MB per simulation run (vs ~10MB with every tick)
- **Smooth motion tracking**: Entities move 10-20m between frames
- **Prediction challenge validation**: 30-tick predictions are trivial, 300-tick predictions are appropriately challenging

See `docs/visualization_results.md` for detailed analysis and `docs/reference_48frame_grid.png` for the reference visualization.

## Project Structure
```
aquascan-gnns/
‚îú‚îÄ‚îÄ configs/            # YAML configuration files
‚îÇ   ‚îî‚îÄ‚îÄ base.yml        # Default configuration
‚îú‚îÄ‚îÄ aquascan/           # Main package
‚îÇ   ‚îú‚îÄ‚îÄ batch/          # Batch generation tools
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate.py # Parallel simulation runner
‚îÇ   ‚îú‚îÄ‚îÄ config/         # Configuration module
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ simulation_config.py  # Central configuration file
‚îÇ   ‚îú‚îÄ‚îÄ dataset/        # Dataset building and processing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_graphs.py  # Convert HDF5 to PyG graphs
‚îÇ   ‚îú‚îÄ‚îÄ io/             # Input/output utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ writer.py   # HDF5 snapshot writer
‚îÇ   ‚îú‚îÄ‚îÄ models/         # Machine learning models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gnn.py      # Graph Neural Network implementation
‚îÇ   ‚îú‚îÄ‚îÄ simulation/     # Core simulation components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ocean_area.py  # Deployment area and node positioning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities.py    # Œµ-nodes, œÉ-nodes, and Œ∏-contact classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensors.py     # Signal simulation and detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ communication.py  # Communication protocols (RPR and SCV)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simulation_loop.py  # Main simulation tick procedure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network_topology.py  # Delaunay-Voronoi mesh topology
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gnn_prediction.py  # Placeholder for future GNN implementation
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hex_grid.py    # Utilities for hexagonal grid coordinates
‚îÇ   ‚îî‚îÄ‚îÄ run_simulation.py  # Entry point script
‚îú‚îÄ‚îÄ scripts/            # Analysis and evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ kalman_eval.py  # Kalman filter baseline evaluation
‚îÇ   ‚îú‚îÄ‚îÄ gnn_train.py    # GNN training script
‚îÇ   ‚îî‚îÄ‚îÄ gnn_eval.py     # GNN evaluation script
‚îú‚îÄ‚îÄ results/            # Output results and benchmarks
‚îÇ   ‚îî‚îÄ‚îÄ kalman_baseline.json  # Kalman filter evaluation results
‚îú‚îÄ‚îÄ data/               # Data storage (HDF5 files, processed graphs)
‚îÇ   ‚îú‚îÄ‚îÄ raw/            # Raw simulation snapshots
‚îÇ   ‚îî‚îÄ‚îÄ processed/      # Processed graph datasets
‚îú‚îÄ‚îÄ tests/              # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_cfg.py     # Tests for configuration functionality
‚îÇ   ‚îî‚îÄ‚îÄ test_kalman_baseline.py  # Tests for Kalman baseline
‚îî‚îÄ‚îÄ requirements.txt    # Project dependencies
```

## Key Components

### Network Architecture
- **Œµ-nodes**: Mobile IoT devices deployed in a hexagonal grid pattern with sonar, hydrophone, and camera sensors
- **œÉ-nodes**: Fixed relay nodes positioned at strategic locations (currently at 19.5km from shore)
- **Œ∏-contacts**: Marine entities (European seabass, Atlantic horse mackerel, Bottlenose dolphin)

### Network Topology
The simulation implements a Delaunay-Voronoi hybrid mesh topology for epsilon node connections:

#### Connection Properties:
- **Permanent connections**: Direct links between nodes within 5km (solid blue lines)
- **Intermittent connections**: Links between nodes 5-10km apart (dashed light blue lines)
- **Connection limits**: Nodes maintain 3-5 connections (min: 3, max: 5)
- **Distance constraints**: All connections respect maximum distance limits (10km absolute maximum)

#### Topology Algorithm:
1. **Initial topology (Delaunay Triangulation)**:
   - Creates mathematically optimal mesh network with triangular structure
   - Generates a base topology that naturally includes redundant paths
   - Maintains maximum connection limit per node (5 connections)

2. **Connection Reinforcement**:
   - Ensures all nodes have at least minimum number of connections (3)
   - Identifies and connects network components with bridges
   - All bridges respect maximum distance constraints (10km)
   - Validates full network connectivity after bridge formation

3. **Dynamic Recalculation (Voronoi Properties)**:
   - Updates intermittent connections (5-10km) every 2 hours
   - Uses Voronoi diagram properties to identify natural neighbors
   - Adapts to nodes moving with ocean currents
   - Efficiently handles network evolution without full recalculation

#### Implementation Features:
- Modular design with injectable topology strategies
- Separate `NetworkTopology` class hierarchy for different approaches
- Automatic component detection and bridge formation
- Balance between energy efficiency and network resilience

### Configuration Management
- **OmegaConf**: The project uses OmegaConf for hierarchical configuration management
- **Configuration Files**: YAML format configuration files in the `configs/` directory
- **Command-line Overrides**: Parameters can be overridden via command-line arguments
- **Testing**: Configuration can be mocked for unit testing

### Protocols
- **Spatiotemporal Contact Volume (SCV)**: Data structure for contact detections
- **Reliable Proximity Relay (RPR)**: Communication protocol between nodes
- **Distributed Observation Buffer (DOB)**: Local and global data storage

### Environmental Dynamics
- **Ocean Currents**: Perlin noise-based vector field
- **Node Drift**: Œµ-nodes follow local current vectors
- **Marine Motion Models**: Brownian motion for fish, sinusoidal for dolphins

## Important Parameters
- **Geographic Area**: 30km √ó 16km (from 6km to 22km offshore)
- **Resolution**: 1km spacing (570 Œµ-nodes)
- **Detection Radius**: 200m (visual indication with color change)
- **Time Step**: 1 second
- **Communication Range**: 10km maximum, 5km optimal
- **Connection Limit**: 3 per node (balancing connectivity and resource usage)

## Setup and Usage

### Installation
1. Clone the repository
2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Unix/macOS
   # or
   venv\Scripts\activate  # On Windows
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running the Simulation

#### Interactive Mode (for visual exploration)
Run the simulation with Bokeh visualization:
```bash
python -m aquascan.run_simulation --show
```

#### Single Run (headless mode)
Run a single simulation without visualization:
```bash
python -m aquascan.run_simulation --headless --ticks 600 --seed 42 --out data/raw/42.h5
```

#### Batch Generation

The batch generator enables parallel simulation runs with different random seeds, creating a corpus of HDF5 files for model training and analysis. Based on visualization analysis, we use **5-tick intervals** (640 seconds) to capture smooth motion while keeping file sizes manageable.

##### Optimized Configuration (Recommended)

Generate dataset with 5-tick snapshot intervals based on visualization findings:
```bash
# Generate 1000 runs with optimized settings (~2GB total)
python -m aquascan.batch.generate --cfg configs/optimal_5tick.yml --runs 1000 --out data/raw_5tick --jobs 8
```

This configuration:
- **Snapshots**: 48 per run (every 5 ticks for 235 ticks)
- **File size**: ~2MB per run
- **Total dataset**: ~2GB for 1000 runs
- **Speed**: x128 (validated through visualization)

See `docs/visualization_results.md` for the analysis that determined these parameters.

##### Standard Configuration

For comparison or different experiments:
```bash
python -m aquascan.batch.generate --cfg configs/base.yml --runs 500 --out data/raw --jobs 8
```

Command-line options for batch generation:
- `--cfg`: Path to the configuration file (use `optimal_5tick.yml` for best results)
- `--runs`: Number of simulations to run (default: 1000)
- `--out`: Output directory for HDF5 files (default: data/raw)
- `--jobs`: Number of parallel workers (default: CPU count)
- `--overwrite`: If set, overwrite existing files (default: skip)
- `--validate`: If set, validate files after generation

**Benefits of the batch generator:**
- **Model training & validation**: GNNs and Kalman filters see thousands of distinct trajectories instead of memorizing a single run
- **Benchmark reproducibility**: Training and evaluation can use identical data without rerunning simulations
- **Experiment speed**: Loading pre-written HDF5s is orders of magnitude faster than running physics simulations
- **Parallel efficiency**: Saturates all CPU cores to reduce generation time
- **Progress tracking**: Real-time progress bar with size estimates

**Example workflow:**
1. Generate optimized dataset:
   ```bash
   python -m aquascan.batch.generate --cfg configs/optimal_5tick.yml --runs 1000 --out data/raw_5tick --jobs 8
   ```

2. Monitor progress:
   ```
   üöÄ [Aquascan Batch Generator - Optimized for 5-tick intervals]
   üìä Storage estimates:
      - Per run: ~2.0 MB
      - Total dataset: ~2.00 GB
   
   Simulations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [15:32<00:00, 1.07sim/s, ok=1000, skip=0, error=0, size_GB=1.95]
   ```

3. Use the dataset for model development:
   ```python
   import h5py
   import glob
   import numpy as np
   
   # Load a batch of simulation files
   simulation_files = glob.glob("data/raw_5tick/*.h5")
   
   # Process each file
   for file_path in simulation_files:
       with h5py.File(file_path, "r") as f:
           # Extract nodes and edges for each tick
           nodes = f["nodes"][:]
           edges = f["edges"][:]
           
           # Use the data for model training, validation, etc.
           # ...
   ```

3. Validate generated files:
   ```bash
   python -m aquascan.batch.generate --cfg configs/base.yml --out data/raw --validate
   ```
   
**Implementation details:**
- Generated HDF5 files are named by their seed number (e.g., `0.h5`, `1.h5`, etc.)
- Each file contains complete simulation snapshots with all nodes and edges
- Files are optimized with chunking and compression for efficient access
- Error handling ensures stability during long batch runs

Command-line options:
- `--cfg`: Path to configuration file (default: configs/base.yml)
- `--headless`: Run without visualization
- `--ticks`: Number of simulation steps to run (headless mode only)
- `--seed`: Random seed for reproducibility
- `--port`: Port for Bokeh server (visual mode only)
- `--show`: Open browser automatically (visual mode only)

### Configuration Files
The default configuration file `configs/base.yml` contains:
```yaml
sim:
  ticks: 600
  seed: 42
  resolution_km: 1.0
  area:
    length_km: 30
    width_km: 16
    shore_offset_km: 6
bokeh:
  port: 5006
  show: false
```

### Running Tests
Execute the unit tests:
```bash
python -m pytest tests/
```

Or run a specific test:
```bash
python -m pytest tests/test_cfg.py
```

### Visualization Tools

The project includes tools for generating visual snapshots from headless simulations to verify data quality and understand entity behavior. See `docs/visualization_results.md` for the analysis that determined optimal data generation parameters.

#### Key Visualization Findings
Based on trajectory analysis with 5-tick intervals:
- **Optimal framerate**: Every 5 ticks (640 seconds) captures smooth motion
- **48 snapshots**: Covers ~8.4 hours of simulation time
- **File size**: ~2MB per run with this configuration
- **Reference grid**: See `visualizations/run_42_20250523_112803/snapshot_grid.png`

#### Generate Simulation Snapshots
Create PNG snapshots at specified intervals during a simulation:
```bash
# Generate optimized 48-frame visualization with trajectories
python scripts/generate_snapshots.py --seed 42 --ticks 235 --interval 5 --output visualizations

# Standard visualization for comparison
python scripts/generate_snapshots.py --seed 42 --ticks 600 --interval 60 --output visualizations
```

Features:
- Entity trajectory lines showing movement paths
- Color-coded entities (each Œ∏-contact has unique color)
- Detection status visualization (bright vs transparent)
- Network topology with connection types
- Ocean current indicators

#### Create Visualization Summaries
Combine snapshots into grids for analysis:
```bash
# Create a 6√ó8 grid view of 48 snapshots
python scripts/visualize_snapshots.py visualizations/run_42_* --grid --cols 6

# Create a 8√ó6 grid for landscape view
python scripts/visualize_snapshots.py visualizations/run_42_* --grid --cols 8
```

#### Quick Test
Run a quick visualization test:
```bash
./test_snapshots.sh  # Generates 2-minute simulation with snapshots every 30 seconds
```

For detailed visualization guide, see `docs/visualization_guide.md`.

## Research Methodology

We developed a custom marine simulation framework in Python using Bokeh for real-time visualization to evaluate predictive models in the absence of real-world passive sensor networks. The simulated environment spans 30√ó16 km, with Œµ-nodes (sensors) deployed in a hexagonal grid and linked via Delaunay triangulation. As the topology evolves, connectivity is updated using Voronoi decomposition. The system models three marine species‚Äîtwo types of fish and cetacea‚Äîusing Brownian and sinusoidal motion patterns to reflect natural variability and enable trajectory verifiability. We generated 500 independent simulations, from which we extracted 40,000 samples comprising time-series sensor contact data. These were split into training (70%), validation (15%), and test sets (15%), with an additional set of synthetic adversarial cases (e.g., biologically implausible speeds or teleporting entities) used to evaluate model robustness and false positive behavior.

The sensing network was encoded as a heterogeneous, partially observed knowledge graph, with nodes representing sensors and edges representing inferred or observed contact transitions. We framed the predictive task as **link prediction**‚Äîlearning to infer missing or future species-specific connections between sensors, effectively modeling plausible migratory paths under degraded observability. We implemented a Graph Neural Network using **GraphSAGE** in PyTorch Geometric, leveraging spatiotemporal node features and localized message passing to infer edges. Predictions were evaluated by comparing inferred links to the ground truth movement graph from simulation, using AUC, precision, and recall as primary metrics. Kalman filtering was implemented as a baseline, applied independently per entity to generate continuous position estimates without graph structure awareness.

## Visualization Features
- Real-time display of node and contact positions
- Visual detection indicator (bright green when detected, grey when not detected)
- Deployment area outline
- Network connections displayed as lines (permanent: solid blue, intermittent: dashed light blue)
- Information panel with simulation statistics
- Start/stop controls
- Simulation speed controls

## Future Development
### Near-term Goals
- Implement data persistence with HDF5
- Add more sophisticated marine entity behavior
- Improve network topology and communication efficiency

### Long-term Goals
- Implement Graph Neural Network (GNN) for prediction
- Add Kalman filter implementation as a baseline
- Create evaluation metrics for prediction accuracy
- Develop integration with real-world data sources

## Notes for Future Developers
- The core simulation is contained in `simulation_loop.py`, which implements the main tick procedure
- Configuration parameters can be adjusted in `simulation_config.py` or via YAML files in `configs/`
- The project uses a modular structure with proper Python package imports
- The simulation supports both headless operation (for data generation/testing) and visual mode
- Detection logic is in `entities.py` in the `EpsilonNode.detect_contact()` method
- OmegaConf is used for configuration management, allowing for hierarchical configurations and overrides
- Unit tests are in the `tests/` directory and use pytest

## Research Question
The central research question for the future development is:
Can GNNs better predict multi-entity motion under communication-constrained sensor networks than Kalman-based baselines?

#### ‚õµ Simulation snapshots
Each headless run writes one **self-contained HDF5** file to `data/raw/`
(e.g. `data/raw/42.h5`).  
This file is the "tape" you replay for model training‚Äîno need to rerun
the sim unless you change the physics.

| Dataset      | Shape        | dtype   | Description                                                  |
|--------------|--------------|---------|--------------------------------------------------------------|
| **/nodes**   | (N_total,)   | compound| Tick-level node states:<br>  ‚Ä¢ `t:int32` ‚Äì tick index<br>  ‚Ä¢ `gid:int32` ‚Äì global ID<br>  ‚Ä¢ `type:uint8` (0 = Œµ, 1 = Œ∏)<br>  ‚Ä¢ `x,y:float32` ‚Äì km coords<br>  ‚Ä¢ `feat:float32[feat_dim]` ‚Äì sensor features |
| **/edges**   | (E_total,)   | compound| Tick-level edges:<br>  ‚Ä¢ `t:int32`, `src:int32`, `dst:int32`<br>  ‚Ä¢ `rel:uint8` (0 = comm, 1 = detect) |
| **/globals** | scalar       | UTF-8 JSON | Run metadata (seed, ticks, resolution, CLI cfg hash)        |

Both `/nodes` and `/edges` are **chunked on `t` and gzip-compressed**,
so you can stream by tick or memory-map the whole thing.

```bash
# Write a snapshot
python -m aquascan.run_simulation \
       --cfg configs/base.yml \
       --headless \
       --ticks 600 \
       --seed 42 \
       --out data/raw/42.h5

# Read a snapshot
import h5py, json
with h5py.File("data/raw/42.h5") as f:
    nodes = f["nodes"][:]        # NumPy structured array
    edges = f["edges"][:]
    meta  = json.loads(f["globals"][()].decode())
```

> **Tip** ‚Äî treat snapshots as immutable; if the schema changes,
> write to a new folder rather than mutating existing files.

## Dataset Module

### Graph Building

The `aquascan.dataset` module converts raw simulation data (HDF5) into graph structures for GNN training:

```bash
python -m aquascan.dataset.build_graphs \
       --raw data/raw \
       --out data/processed \
       --context 60 \  # Context window length in ticks
       --horizon 30 \  # Future prediction window length
       --split 0.7 0.15 0.15 \  # train/val/test split
       --adv_fraction 0.05  # Fraction of adversarial examples
```

### Data Structure

- **Raw data**: `data/raw/*.h5` - HDF5 files from simulation containing nodes and edges
- **Processed data**: 
  - `data/processed/train.pt` - Training graphs
  - `data/processed/val.pt` - Validation graphs
  - `data/processed/test.pt` - Test graphs
  - `data/processed/adversarial.pt` - Adversarial examples
  - `data/processed/meta.json` - Dataset metadata

### Graph Format

Each graph is a `HeteroData` object (PyTorch Geometric) with:
- Nodes: `epsilon` (sensors) and `theta` (marine entities)
- Node features: `[x, y, Œîx, Œîy]` (position and velocity)
- Edge types:
  - `epsilon -> communicates -> epsilon`: Communication links
  - `epsilon -> detects -> theta`: Detection events in context window
  - `epsilon -> will_detect -> theta`: Target edges for prediction (binary labels)

### Developer Notes

- Large binary files (`.h5`, `.pt`) are tracked with Git LFS
- Processing large datasets can be slow (expect minutes to hours depending on size)
- Required dependencies: `torch`, `torch_geometric`, `h5py`, `numpy`

## Kalman Filter Baseline

### Overview

The Kalman filter baseline provides a classical physics-based benchmark for marine entity detection prediction. It implements a constant-velocity Kalman filter that predicts future Œµ-Œ∏ detection links based on historical position data.

### Implementation

**Algorithm Details:**
- **State Vector**: `[x, y, vx, vy]·µÄ` (position and velocity)
- **Motion Model**: Constant velocity with process noise
- **Context Window**: Uses last `context_len` positions to initialize/update filter
- **Prediction Horizon**: Predicts `horizon_len` steps ahead (typically 1 step)
- **Edge Reconstruction**: Adds detection edges for any Œµ-Œ∏ pair with predicted distance ‚â§ 100m

**Key Features:**
- Per-Œ∏ Kalman filter for each marine entity
- Classical constant-velocity motion model
- Distance-based edge reconstruction
- Evaluation using AUC, precision, and recall metrics

### Usage

#### Running the Baseline Evaluation

```bash
# Activate virtual environment
source venv/bin/activate

# Run Kalman filter evaluation on test set
python scripts/kalman_eval.py
```

This will:
1. Load processed test graphs from `data/processed/test.pt`
2. Run Kalman filter prediction for each marine entity
3. Reconstruct detection edges based on 100m distance threshold
4. Calculate AUC, precision, and recall metrics
5. Save results to `results/kalman_baseline.json`

#### Expected Output

```
Running Kalman Filter Baseline Evaluation
Processing 90 graphs...
Processing graph 1/90
...
Total predictions: 564300
Positive labels: 90.0
Predicted positive: 54.0
Results:
  AUC: 0.8000
  Precision: 1.0000
  Recall: 0.6000

Results saved to results/kalman_baseline.json
```

### Results Interpretation

The baseline achieves:
- **AUC: 0.80** - Good discrimination between true and false detections
- **Precision: 1.00** - No false positives (conservative predictions)
- **Recall: 0.60** - Captures 60% of actual future detections

These results demonstrate that a simple physics-based approach can achieve reasonable performance, providing a solid baseline for comparison with more sophisticated GNN approaches.

### Files and Structure

- **`scripts/kalman_eval.py`**: Main evaluation script
  - Implements constant-velocity Kalman filter
  - Loads test data and generates predictions
  - Calculates evaluation metrics
  - Saves results to JSON

- **`results/kalman_baseline.json`**: Evaluation results
  ```json
  {
    "AUC": 0.8,
    "Precision": 1.0,
    "Recall": 0.6
  }
  ```

- **`tests/test_kalman_baseline.py`**: Unit tests
  - Validates result file structure
  - Checks data loading functionality
  - Ensures metric values are reasonable

### Running Tests

```bash
# Run Kalman baseline tests
python -m pytest tests/test_kalman_baseline.py -v

# Run all tests including Kalman baseline
python -m pytest tests/ -v
```

### Performance

- **Runtime**: ~10 seconds for full test set evaluation
- **Memory**: Minimal memory usage (pure NumPy operations)
- **Scalability**: Linear with number of graphs and entities

### Technical Notes

**Kalman Filter Implementation:**
- Uses NumPy for efficient matrix operations
- Process noise covariance tuned for marine entity motion
- Handles variable numbers of context observations
- Robust to missing or incomplete data

**Edge Reconstruction:**
- Distance threshold: 100m (0.1km)
- Binary classification: within threshold = detection
- Evaluates all possible Œµ-Œ∏ pairs per graph
- Highly imbalanced dataset (90 positive vs 564,210 negative labels)

**Evaluation Methodology:**
- Uses scikit-learn metrics for standard evaluation
- Binary threshold based on distance for precision/recall
- AUC calculated using predicted probabilities
- Consistent with GNN evaluation framework

## Heterogeneous GraphSAGE Implementation

### Overview

Our GNN implementation leverages **Heterogeneous GraphSAGE (HeteroGraphSAGE)** for marine entity detection prediction. The approach treats the sensor network as a heterogeneous graph where different node types (sensors vs. marine entities) have distinct semantic roles, and learns to predict future detection links through structured message passing.

### Mathematical Foundation

#### Heterogeneous Graph Representation

We model the marine sensor network as a heterogeneous graph **G = (V, E, T, R)** where:

- **V**: Set of nodes partitioned into types T = {Œµ, Œ∏}
  - **Œµ-nodes**: Sensor nodes (|V_Œµ| ‚âà 570)
  - **Œ∏-nodes**: Marine entities (|V_Œ∏| ‚âà 15)

- **E**: Set of edges partitioned into relations R = {communicates, detects, will_detect}
  - **Œµ ‚Üí^communicates Œµ**: Communication links between sensors
  - **Œµ ‚Üí^detects Œ∏**: Historical detection events (context window)
  - **Œµ ‚Üí^will_detect Œ∏**: Future detection targets (prediction labels)

- **Node Features**: Each node v ‚àà V has features **x_v ‚àà ‚Ñù^4**
  - **x_v = [x, y, Œîx, Œîy]^T**: Position (x,y) and velocity (Œîx, Œîy)

#### HeteroGraphSAGE Architecture

Our model implements a 3-layer heterogeneous GraphSAGE with the following mathematical formulation:

**Layer ‚Ñì Message Passing:**

For each edge type r ‚àà R and target node type t:

```
h_v^{(‚Ñì+1)} = œÉ(W_r^{(‚Ñì)} ¬∑ CONCAT(h_v^{(‚Ñì)}, AGG_r({h_u^{(‚Ñì)} : u ‚àà N_r(v)})))
```

Where:
- **h_v^{(‚Ñì)}**: Hidden representation of node v at layer ‚Ñì
- **N_r(v)**: Neighbors of v connected via edge type r
- **AGG_r**: Aggregation function (mean pooling) for relation r
- **W_r^{(‚Ñì)} ‚àà ‚Ñù^{2d√ód}**: Learnable weight matrix for relation r
- **œÉ**: ReLU activation function
- **d = 64**: Hidden dimension

**Specific Relations:**

1. **Œµ-Œµ Communication**: Updates sensor embeddings based on neighboring sensors
   ```
   h_Œµ^{(‚Ñì+1)} = œÉ(W_comm^{(‚Ñì)} ¬∑ CONCAT(h_Œµ^{(‚Ñì)}, MEAN({h_u^{(‚Ñì)} : u ‚àà N_comm(Œµ)})))
   ```

2. **Œµ-Œ∏ Detection**: Updates both sensor and entity embeddings based on detection history
   ```
   h_Œµ^{(‚Ñì+1)} = œÉ(W_detect^{(‚Ñì)} ¬∑ CONCAT(h_Œµ^{(‚Ñì)}, MEAN({h_Œ∏^{(‚Ñì)} : Œ∏ ‚àà N_detect(Œµ)})))
   h_Œ∏^{(‚Ñì+1)} = œÉ(W_detect^{(‚Ñì)} ¬∑ CONCAT(h_Œ∏^{(‚Ñì)}, MEAN({h_Œµ^{(‚Ñì)} : Œµ ‚àà N_detect(Œ∏)})))
   ```

**Input Projection:**

Each node type has a dedicated input projection to map raw features to hidden space:
```
h_Œµ^{(0)} = W_Œµ^{input} ¬∑ x_Œµ + b_Œµ^{input}
h_Œ∏^{(0)} = W_Œ∏^{input} ¬∑ x_Œ∏ + b_Œ∏^{input}
```

**Batch Normalization:**

Each layer applies batch normalization per node type before activation:
```
h_v^{(‚Ñì+1)} = œÉ(BatchNorm_t(W_r^{(‚Ñì)} ¬∑ CONCAT(...)))
```

Where **t** is the node type of **v**.

#### Link Prediction via Bilinear Scoring

**Final Prediction:**

After L=3 layers, we obtain final embeddings **h_Œµ^{(L)}** and **h_Œ∏^{(L)}**. Link prediction uses element-wise dot product scoring:

```
score(Œµ_i, Œ∏_j) = h_{Œµ_i}^{(L)} ‚äô h_{Œ∏_j}^{(L)} = Œ£_{k=1}^d h_{Œµ_i,k}^{(L)} ¬∑ h_{Œ∏_j,k}^{(L)}
```

**Prediction Probability:**
```
p(Œµ_i will_detect Œ∏_j) = sigmoid(score(Œµ_i, Œ∏_j))
```

### Design Rationale

#### Why Heterogeneous GraphSAGE?

1. **Semantic Distinction**: Sensors and marine entities have fundamentally different roles and motion patterns. Heterogeneous graphs naturally capture this via distinct node types and relation-specific message passing.

2. **Scalable Architecture**: GraphSAGE's sampling-based approach scales to large graphs (570+ nodes) while maintaining expressiveness through neighborhood aggregation.

3. **Inductive Learning**: Unlike spectral GNNs, GraphSAGE can generalize to unseen network topologies, crucial for dynamic sensor deployments.

#### Why Element-wise Dot Product Scoring?

The element-wise dot product **h_Œµ ‚äô h_Œ∏** captures **semantic similarity** between sensor and entity embeddings:

- **Computational Efficiency**: O(d) per edge vs. O(d¬≤) for full bilinear forms
- **Geometric Interpretation**: Measures alignment between embedding vectors in latent space
- **Empirical Performance**: Widely successful in knowledge graph and recommender systems

#### Loss Function and Training Objective

**Binary Cross-Entropy with Logits:**
```
L = -1/|E_pred| Œ£_{(Œµ,Œ∏) ‚àà E_pred} [y_ŒµŒ∏ ¬∑ log(p_ŒµŒ∏) + (1-y_ŒµŒ∏) ¬∑ log(1-p_ŒµŒ∏)]
```

Where:
- **E_pred**: Set of sensor-entity pairs for prediction
- **y_ŒµŒ∏ ‚àà {0,1}**: Binary detection label (1 if detection occurs)
- **p_ŒµŒ∏**: Predicted probability from sigmoid(score(Œµ,Œ∏))

**Class Imbalance Handling:**

The dataset is highly imbalanced (~90 positive vs. 564,300 negative labels). The model implicitly learns this distribution, achieving excellent AUC (‚â•0.99) by learning to rank detection probabilities effectively.

### Implementation Details

#### Model Architecture:
- **Input Dimension**: 4 (position + velocity features)
- **Hidden Dimension**: 64 
- **Layers**: 3 GraphSAGE layers
- **Activation**: ReLU with batch normalization
- **Aggregation**: Mean pooling

#### Training Configuration:
- **Optimizer**: Adam (lr=1e-3, weight_decay=1e-4)
- **Loss**: BCEWithLogitsLoss
- **Batch Size**: 8 graphs
- **Early Stopping**: 5 epochs without validation AUC improvement
- **Device**: CPU/GPU adaptive

#### Evaluation Metrics:
- **AUC**: Area Under ROC Curve (primary metric)
- **Precision**: True Positives / (True Positives + False Positives)
- **Recall**: True Positives / (True Positives + False Negatives)

### Performance Analysis

**Achieved Results:**
- **Validation AUC**: 1.0000 (target: ‚â•0.83) ‚úÖ
- **Test AUC**: 1.0000 (target: ‚â•0.82) ‚úÖ
- **Precision @ œÑ=0.9944**: 0.8036 (‚â•0.8) ‚úÖ
- **Recall**: 1.0 (perfect detection of true positives)

**Comparison to Kalman Baseline:**
- **GNN AUC**: 1.0000 vs. **Kalman AUC**: 0.80 (+25.0% improvement)
- **GNN Recall**: 1.0 vs. **Kalman Recall**: 0.6 (+66.7% improvement)

The GNN significantly outperforms the physics-based Kalman filter by learning complex spatiotemporal patterns in the sensor-entity interaction graph that are not captured by simple motion models.

#### Class Imbalance Challenge

In each graph window we evaluate every possible Œµ-Œ∏ pair as a candidate "future-detection" edge, but only a handful of those pairs will actually be detected during the horizon ticks. In the current test split that works out to 90 positives versus roughly 564,000 negatives‚Äîabout one positive for every 6,300 negatives. This extreme class-imbalance means that most learning signal comes from the abundant negative class. Ranking metrics such as ROC-AUC are largely insensitive to the skew (hence the near-perfect AUC of 0.999), yet threshold-based metrics like precision suffer when we apply a default 0.5 cutoff: the model produces many false positives simply because positives are so rare. Mitigating the imbalance therefore calls for threshold tuning (e.g., choosing œÑ where precision meets a desired level) or loss re-weighting, rather than additional training epochs.

**Technical Solutions Implemented:**

We addressed the extreme class imbalance through two complementary approaches. First, we implemented **class-balanced loss weighting** by setting `pos_weight = neg_count/pos_count ‚âà 6,269` in the BCEWithLogitsLoss function, effectively upweighting the sparse positive examples during training to prevent the model from becoming biased toward the majority class. Second, we employed **precision-recall curve analysis** to find the optimal operating point: using `sklearn.metrics.precision_recall_curve`, we identified œÑ = 0.9944 as the threshold that achieves precision ‚â• 0.8 while maintaining perfect recall. This threshold-tuning approach leverages the model's excellent ranking ability (AUC ‚âà 1.0) to achieve practical precision-recall trade-offs. The combination reduced false positive predictions from 117,660 at the default 0.5 threshold to just 112 at the optimal threshold‚Äîa 99.9% reduction while maintaining 80.4% precision and 100% recall.

### Graph Construction Pipeline

**Context Window Approach:**
1. **Historical Context**: Use last 60 ticks of detections as Œµ‚ÜíŒ∏ edges
2. **Future Horizon**: Predict detections 30 ticks ahead as Œµ‚Üíwill_detect‚ÜíŒ∏ labels
3. **Communication Graph**: Static Œµ‚ÜíŒµ edges based on Delaunay-Voronoi topology

**Feature Engineering:**
- **Positional Features**: Raw (x,y) coordinates in km
- **Velocity Features**: Finite difference Œîx, Œîy over time steps
- **Normalization**: Features scaled to [0,1] range for stable training

This implementation represents a state-of-the-art approach to spatiotemporal link prediction in heterogeneous sensor networks, leveraging the latest advances in geometric deep learning for marine monitoring applications.
